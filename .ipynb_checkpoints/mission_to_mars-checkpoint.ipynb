{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pymongo\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "from splinter.exceptions import ElementDoesNotExist\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import re\n",
    "import time\n",
    "\n",
    "#Dictionary to collect data\n",
    "mars = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = init_browser()\n",
    "\n",
    "# URL of page to be scraped\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = bs(response.text, 'lxml')\n",
    "    #print(soup.prettify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the results, then determine element that contains sought info\n",
    "# results are returned as an iterable list\n",
    "\n",
    "#Thanks to https://stackoverflow.com/questions/46468030/how-select-class-div-tag-in-splinter\n",
    "\n",
    "# Put together link to top story\n",
    "link = [a['href'] for a in soup.select(\"a[href*=news]\")][0]\n",
    "base = \"https://mars.nasa.gov/news\"\n",
    "url = urllib.parse.urljoin(base,link)\n",
    "\n",
    "# Fetch headline and text  \n",
    "browser.visit(url)\n",
    "news_title = browser.find_by_tag('h1').value\n",
    "#This next is fancier than it needs to be, but I was trying to figure out how to get rid of captions in the middle\n",
    "#Short of picking out individual p elements, which won't work once the site is updated, I don't see how\n",
    "news_p = browser.find_by_css('div[id=\"primary_column\"]').text\n",
    "news_p = news_p.replace('\\n', ' ')\n",
    "#Remaining backslashes removed in actual app\n",
    "\n",
    "# Add title and text to data dictionary \n",
    "mars[\"news_title\"] = news_title\n",
    "mars[\"news_p\"] = news_p\n",
    "  \n",
    "news_title\n",
    "news_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to GitHub for the fancybox help, I was really stuck. Didn't realize \"fancybox\" could be a class. \n",
    "#https://github.com/ZL14E161110/HW13---Web-Scraping-and-Document-Databases\n",
    "#Didn't look at the rest of it, although I'm sure it's brilliant. \n",
    "\n",
    "#Assumes the latest Mars image will always be the one in the first position of the grid...\n",
    "#Note 0 is the overall featured image, which is not necessarily of Mars.\n",
    "\n",
    "# Visit site and make soup\n",
    "url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "response = requests.get(url)\n",
    "soup = bs(response.text, 'lxml')\n",
    "\n",
    "#Get image \n",
    "partial_address = soup.find_all('a', class_='fancybox')[1].get('data-fancybox-href').strip()\n",
    "base = \"https://www.jpl.nasa.gov\"\n",
    "featured_image_url = urllib.parse.urljoin(base, partial_address)\n",
    "\n",
    "#Get brief image description\n",
    "image_description = soup.find_all('a', class_='fancybox')[1].get('data-description').strip()\n",
    "\n",
    "#Add url and description to dictionary\n",
    "mars[\"featured_image_url\"] = featured_image_url\n",
    "mars[\"image_description\"] = image_description\n",
    "\n",
    "mars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://space-facts.com/mars/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(url)\n",
    "#tables = pd.read_html(url, header=None, index_col=0)\n",
    "type(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_facts_df = tables[0]\n",
    "mars_facts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_facts_html_table = mars_facts_df.to_html(header=False, border=False, index=False)\n",
    "mars_facts_html_table = mars_facts_html_table.replace('\\n', '')\n",
    "mars_facts_html_table\n",
    "#print(mars_facts_html_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add table to dictionary\n",
    "mars[\"table\"] = mars_facts_html_table\n",
    "mars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#navigate to starting page\n",
    "browser=init_browser()\n",
    "url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make soup\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = bs(response.text, 'lxml')\n",
    "#print(soup.prettify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere Enhanced',\n",
       "  'thumbnail_url': 'https://astrogeology.usgs.gov/search/map/Mars/Viking/cerberus_enhanced'},\n",
       " {'title': 'Schiaparelli Hemisphere Enhanced',\n",
       "  'thumbnail_url': 'https://astrogeology.usgs.gov/search/map/Mars/Viking/schiaparelli_enhanced'},\n",
       " {'title': 'Syrtis Major Hemisphere Enhanced',\n",
       "  'thumbnail_url': 'https://astrogeology.usgs.gov/search/map/Mars/Viking/syrtis_major_enhanced'},\n",
       " {'title': 'Valles Marineris Hemisphere Enhanced',\n",
       "  'thumbnail_url': 'https://astrogeology.usgs.gov/search/map/Mars/Viking/valles_marineris_enhanced'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(url)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = bs(response.text, 'lxml')\n",
    "\n",
    "#Find titles and links for full-scale images and information pages\n",
    "\n",
    "base = \"https://astrogeology.usgs.gov\"\n",
    "hemisphere_image_urls = []\n",
    "thumbnail_urls = []\n",
    "\n",
    "for link in soup.find_all(class_='itemLink product-item'):\n",
    "    partial = link.get('href')   \n",
    "    thumbnail_url = urllib.parse.urljoin(base, partial)\n",
    "    browser.visit(thumbnail_url)\n",
    "    xpath = \"//a[@class='open-toggle']\"\n",
    "    switch = browser.find_by_xpath(xpath)\n",
    "    switch.click()\n",
    "    html = browser.html\n",
    "    soup = bs(html, 'lxml')\n",
    "    imgs = soup.find(class_='downloads')\n",
    "    title = soup.find(class_ = 'title').get_text()\n",
    "    images=browser.find_by_xpath('/html/body/div[1]/div[1]/div[2]/img')  \n",
    "    img_url =  images[\"src\"]  \n",
    "    thumbnail_urls.append({'title':title, 'thumbnail_url':thumbnail_url})\n",
    "    hemisphere_image_urls.append({'title':title, 'img_url':img_url})\n",
    "\n",
    "browser.quit()\n",
    "mars[\"hemisphere_image_urls\"] = hemisphere_image_urls\n",
    "mars[\"thumbnail_urls\"] = thumbnail_urls\n",
    "thumbnail_urls\n",
    "#hemisphere_image_urls\n",
    "#return mars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - MongoDB and Flask Application\n",
    "\n",
    "Use MongoDB with Flask templating to create a new HTML page that displays all of the information that was scraped from the URLs above.\n",
    "\n",
    "* Start by converting your Jupyter notebook into a Python script called `scrape_mars.py` with a function called `scrape` that will execute all of your scraping code from above and return one Python dictionary containing all of the scraped data.\n",
    "\n",
    "* Next, create a route called `/scrape` that will call your `scrape_mars.scrape` function. Note that you'll have to import `scrape_mars.py`. \n",
    "\n",
    "  * Store the dictionary that gets returned to your MongoDB.\n",
    "\n",
    "* Create an index route `/` that will query your Mongo database and pass the Mars data into an HTML template to be displayed. \n",
    "\n",
    "* Create a template HTML file called `index.html` that will take the dictionary of Mars data and display its values in the appropriate HTML elements. Use the following as a guide for what the final product should look like, but feel free to create your own design.\n",
    "\n",
    "![final_app_part1.png](Images/final_app_part1.png)\n",
    "![final_app_part2.png](Images/final_app_part2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
