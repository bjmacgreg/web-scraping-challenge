{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pymongo\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "from splinter.exceptions import ElementDoesNotExist\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import re\n",
    "import time\n",
    "\n",
    "#Dictionary to collect data\n",
    "mars = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = init_browser()\n",
    "\n",
    "# URL of page to be scraped\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = bs(response.text, 'lxml')\n",
    "    #print(soup.prettify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the results, then determine element that contains sought info\n",
    "# results are returned as an iterable list\n",
    "\n",
    "# Put together link to top story\n",
    "link = [a['href'] for a in soup.select(\"a[href*=news]\")][0]\n",
    "base = \"https://mars.nasa.gov/news\"\n",
    "url = urllib.parse.urljoin(base,link)\n",
    "\n",
    "# Fetch headline and text  \n",
    "browser.visit(url)\n",
    "news_title = browser.find_by_tag('h1').value\n",
    "news_p = browser.find_by_id('primary_column').value\n",
    "news_p = news_p.replace('\\n', ' ')\n",
    "#tried a million ways to get rid of remaining \\ (see old notebook), no luck yet\n",
    "#But they are gone in the Chrome versio, thanks to whoever...\n",
    "#story = {'news_title': news_title,'news_p': news_p}\n",
    "\n",
    "# Add title and text to data dictionary \n",
    "mars[\"news_title\"] = news_title\n",
    "mars[\"news_p\"] = news_p\n",
    "  \n",
    "news_title\n",
    "news_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to GitHub for the fancybox help, I was really stuck. Didn't realize \"fancybox\" could be a class. \n",
    "#https://github.com/ZL14E161110/HW13---Web-Scraping-and-Document-Databases\n",
    "#Didn't look at the rest of it, although I'm sure it's brilliant. \n",
    "\n",
    "#Assumes the latest Mars image will always be the one in the first position of the grid...\n",
    "#Note 0 is the overall featured image, which is not necessarily of Mars.\n",
    "\n",
    "# Visit site and make soup\n",
    "url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "response = requests.get(url)\n",
    "soup = bs(response.text, 'lxml')\n",
    "\n",
    "#Get image \n",
    "partial_address = soup.find_all('a', class_='fancybox')[1].get('data-fancybox-href').strip()\n",
    "base = \"https://www.jpl.nasa.gov\"\n",
    "featured_image_url = urllib.parse.urljoin(base, partial_address)\n",
    "#featured_image_url\n",
    "\n",
    "#Add url to dictionary\n",
    "mars[\"featured_image_url\"] = featured_image_url\n",
    "\n",
    "mars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://space-facts.com/mars/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(url)\n",
    "#tables = pd.read_html(url, header=None, index_col=0)\n",
    "type(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_facts_df = tables[0]\n",
    "mars_facts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_facts_html_table = mars_facts_df.to_html(header=False, border=False, index=False)\n",
    "mars_facts_html_table = mars_facts_html_table.replace('\\n', '')\n",
    "mars_facts_html_table\n",
    "#print(mars_facts_html_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add table to dictionary\n",
    "mars[\"table\"] = mars_facts_html_table\n",
    "mars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#navigate to starting page\n",
    "browser=init_browser()\n",
    "url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make soup\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = bs(response.text, 'lxml')\n",
    "#print(soup.prettify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Find titles and links for full-scale images\n",
    "\n",
    "#OLD PATH TO \n",
    "\n",
    "#Find titles and links for full-scale images\n",
    "\n",
    "#title_list = []\n",
    "#img_url_list = []\n",
    "base = \"https://astrogeology.usgs.gov\"\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "\n",
    "for link in soup.find_all(class_='itemLink product-item'):\n",
    "    partial = link.get('href')    \n",
    "    thumbnail_url = urllib.parse.urljoin(base, partial)\n",
    "    browser.visit(thumbnail_url)\n",
    "    xpath = \"//a[@class='open-toggle']\"\n",
    "    switch = browser.find_by_xpath(xpath)\n",
    "    switch.click()\n",
    "    html = browser.html\n",
    "    soup = bs(html, 'lxml')\n",
    "#    print(soup.prettify)    \n",
    "\n",
    "    \n",
    "    \n",
    "#    imgs = soup.find(class_='downloads')\n",
    "    html = browser.html\n",
    "    soup = bs(html, 'lxml')\n",
    "    title = soup.find(class_ = 'title').get_text()\n",
    "    img_url = [a['href'] for a in soup.select('a:contains(Original)')]\n",
    "    img_url = str(*img_url)\n",
    "    hemisphere_image_urls.append({'title':title, 'img_url':img_url})\n",
    "browser.quit()\n",
    "hemisphere_image_urls\n",
    "\n",
    "#mars[\"hemisphere_image_urls\"] = hemisphere_image_urls\n",
    "#mars\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#    images = soup.a.img['src']    \n",
    "#    print(images)\n",
    "    \n",
    "#    response = http.request('GET', url)\n",
    "#    soup = BSHTML(response.data, \"html.parser\")\n",
    "    \n",
    "#    urls = soup.find_all('img',{'class':\"wide-image\"}) \n",
    "#    urls = soup.find_all('img',\"wide-image\") \n",
    "#    print(urls)\n",
    "#    urls = soup.select('img[class=\"wide-image\"]')\n",
    "#    for url in urls:\n",
    "        #print image source\n",
    "#        print(url)        \n",
    "#        print(image['src'])\n",
    "        #print alternate text\n",
    "        #print(image['alt'])    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#    all_images = soup.findAll('img')\n",
    "\n",
    "#    for image in all_images:\n",
    "        #print image source\n",
    "#        print(image['src'])\n",
    "        #print alternate text\n",
    "#        print(image['alt'])\n",
    "    \n",
    "    \n",
    "#    images = soup.findAll('img')\n",
    "\n",
    "#    for image in images:\n",
    "#        print (image['src'])\n",
    "#    title = soup.find(class_ = 'title').get_text()\n",
    "#    partial_address = soup.select('img', 'wide-image', 'src')\n",
    "#    thumbnail_url = urllib.parse.urljoin(base, partial_address)\n",
    "#    img_url = [a['href'] for a in soup.select('a:contains(Original)')]\n",
    "#    img_url = str(*img_url)\n",
    "#    thumbnail_url\n",
    "#    hemisphere_image_urls.append({'title':title, 'img_url':img_url})\n",
    "#browser.quit()\n",
    "#hemisphere_image_urls\n",
    "\n",
    "#mars[\"hemisphere_image_urls\"] = hemisphere_image_urls\n",
    "#mars\n",
    "\n",
    "#print(image_url_list)\n",
    "\n",
    "\n",
    "#    title_list.append(title)\n",
    "#    image_url_list = img_url_list.append(*img_url)\n",
    "#    title_list\n",
    "#    print(title, *img_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere Enhanced',\n",
       "  'img_url': 'https://astrogeology.usgs.gov/cache/images/f5e372a36edfa389625da6d0cc25d905_cerberus_enhanced.tif_full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere Enhanced',\n",
       "  'img_url': 'https://astrogeology.usgs.gov/cache/images/3778f7b43bbbc89d6e3cfabb3613ba93_schiaparelli_enhanced.tif_full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere Enhanced',\n",
       "  'img_url': 'https://astrogeology.usgs.gov/cache/images/555e6403a6ddd7ba16ddb0e471cadcf7_syrtis_major_enhanced.tif_full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere Enhanced',\n",
       "  'img_url': 'https://astrogeology.usgs.gov/cache/images/b3c7c6c9138f57b4756be9b9c43e3a48_valles_marineris_enhanced.tif_full.jpg'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OLD PATH TO \n",
    "\n",
    "url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(url)\n",
    "time.sleep(10) \n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = bs(response.text, 'lxml')\n",
    "\n",
    "#Find titles and links for full-scale images\n",
    "\n",
    "base = \"https://astrogeology.usgs.gov\"\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "for link in soup.find_all(class_='itemLink product-item'):\n",
    "    partial = link.get('href')    \n",
    "    thumbnail_url = urllib.parse.urljoin(base, partial)\n",
    "    browser.visit(thumbnail_url)\n",
    "    xpath = \"//a[@class='open-toggle']\"\n",
    "    switch = browser.find_by_xpath(xpath)\n",
    "    switch.click()\n",
    "    html = browser.html\n",
    "    soup = bs(html, 'lxml')\n",
    "    imgs = soup.find(class_='downloads')\n",
    "    title = soup.find(class_ = 'title').get_text()\n",
    "    images=browser.find_by_xpath('/html/body/div[1]/div[1]/div[2]/img')  \n",
    "    img_url =  images[\"src\"]  \n",
    "    hemisphere_image_urls.append({'title':title, 'img_url':img_url})\n",
    "browser.quit()\n",
    "hemisphere_image_urls\n",
    "mars[\"hemisphere_image_urls\"] = hemisphere_image_urls\n",
    "\n",
    "#return mars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - MongoDB and Flask Application\n",
    "\n",
    "Use MongoDB with Flask templating to create a new HTML page that displays all of the information that was scraped from the URLs above.\n",
    "\n",
    "* Start by converting your Jupyter notebook into a Python script called `scrape_mars.py` with a function called `scrape` that will execute all of your scraping code from above and return one Python dictionary containing all of the scraped data.\n",
    "\n",
    "* Next, create a route called `/scrape` that will call your `scrape_mars.scrape` function. Note that you'll have to import `scrape_mars.py`. \n",
    "\n",
    "  * Store the dictionary that gets returned to your MongoDB.\n",
    "\n",
    "* Create an index route `/` that will query your Mongo database and pass the Mars data into an HTML template to be displayed. \n",
    "\n",
    "* Create a template HTML file called `index.html` that will take the dictionary of Mars data and display its values in the appropriate HTML elements. Use the following as a guide for what the final product should look like, but feel free to create your own design.\n",
    "\n",
    "![final_app_part1.png](Images/final_app_part1.png)\n",
    "![final_app_part2.png](Images/final_app_part2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
